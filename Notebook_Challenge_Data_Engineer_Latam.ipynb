{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Data Engineer LATAM - Juan Felipe Hurtado\n",
    "\n",
    "- El archivo json se tiene en el directorio raiz de este archivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importaciones globales del reto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ujson as json\n",
    "import emoji\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "from datetime import datetime\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Las top 10 fechas donde hay más tweets. \n",
    "Mencionar el usuario (username) que más publicaciones tiene por cada uno de esos días.\n",
    "- Enfoque: Tiempo de ejecucion optimizado.\n",
    "\n",
    "- Descripción:\n",
    "Este código define una función llamada q1_time que procesa un archivo. Si existe un archivo Parquet más reciente que el archivo original, lo lee; de lo contrario, lee el archivo JSON, realiza manipulaciones de datos y lo guarda como Parquet. Luego, identifica las 10 fechas con más apariciones en el archivo original y, para cada una de ellas, encuentra el nombre de usuario con más apariciones en esa fecha. El resultado final es una lista de tuplas que contienen la fecha y el nombre de usuario correspondiente para las 10 fechas principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    # Divide el file_path en nombre de archivo y extensión\n",
    "    file_name = os.path.splitext(file_path)\n",
    "    \n",
    "    # Crea el nombre del archivo Parquet\n",
    "    parquet_file = file_name + '.parquet'\n",
    "    \n",
    "    # Verifica si existe el archivo Parquet y si es más reciente que el archivo original\n",
    "    if os.path.exists(parquet_file) and os.path.getmtime(parquet_file) >= os.path.getmtime(file_path):\n",
    "        # Lee el archivo Parquet si cumple la condición\n",
    "        df = pd.read_parquet(parquet_file)\n",
    "    else:\n",
    "        # Lee el archivo JSON y realiza manipulaciones de datos si no cumple la condición\n",
    "        df = pd.read_json(file_path, lines=True)\n",
    "        df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "        df['username'] = df['user'].apply(lambda x: x.get('username'))\n",
    "        del df['user']\n",
    "        df.to_parquet(parquet_file, index=False)\n",
    "\n",
    "    # Agrupa por fecha y nombre de usuario, cuenta y obtiene la fila con el máximo conteo por fecha\n",
    "    top_users_df = df.groupby(['date', 'username']).size().reset_index(name='count')\n",
    "    top_users_df = top_users_df.loc[top_users_df.groupby('date')['count'].idxmax()]\n",
    "    \n",
    "    # Encuentra las 10 fechas con más apariciones y su recuento\n",
    "    top_dates_df = df['date'].value_counts().nlargest(10).reset_index()\n",
    "    \n",
    "    # Crea una lista de tuplas con la fecha y el nombre de usuario correspondiente para las 10 fechas principales\n",
    "    result = [(row['index'], top_users_df[top_users_df['date'] == row['index']]['username'].values[0]) for _, row in top_dates_df.iterrows()]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Enfoque: Memoria en uso optimizada\n",
    " - Descripción: Este código define una función llamada q1_memory que procesa un archivo de texto. Lee cada línea del archivo, interpreta cada línea como un objeto JSON que representa un tweet, y realiza un seguimiento de la fecha y el nombre de usuario más frecuentes para cada fecha. Luego, encuentra las 10 fechas con mayor número de usuarios únicos y devuelve una lista de tuplas que contienen la fecha y el nombre de usuario más frecuente para cada una de esas fechas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    # Crear un diccionario anidado para contar usuarios por fecha\n",
    "    usuarios_por_fecha = defaultdict(lambda: defaultdict(int))\n",
    "    # Abrir el archivo en modo lectura\n",
    "    with open(file_path, \"r\") as archivo:\n",
    "        # Iterar sobre cada línea en el archivo\n",
    "        for linea in archivo:\n",
    "            # Cargar cada línea como un objeto JSON\n",
    "            tweet = json.loads(linea)\n",
    "            # Extraer la fecha del tweet y convertirla en un objeto de fecha\n",
    "            fecha_tweet_str = tweet['date'].split(\"T\")[0]\n",
    "            fecha_tweet = datetime.strptime(fecha_tweet_str, '%Y-%m-%d').date()\n",
    "            # Obtener el nombre de usuario del tweet\n",
    "            nombre_usuario = tweet['user']['username']\n",
    "            # Actualizar el contador de usuarios por fecha\n",
    "            usuarios_por_fecha[fecha_tweet][nombre_usuario] += 1\n",
    "    # Inicializar una lista vacía para almacenar las fechas y usuarios principales\n",
    "    fechas_usuarios_principales = []\n",
    "    # Iterar a través de las fechas y sus recuentos de usuarios\n",
    "    for fecha, recuentos_usuarios in usuarios_por_fecha.items():\n",
    "        # Encontrar el usuario con el recuento máximo para cada fecha\n",
    "        usuario_principal = max(recuentos_usuarios, key=recuentos_usuarios.get)\n",
    "        # Agregar la fecha y el usuario principal a la lista\n",
    "        fechas_usuarios_principales.append((fecha, usuario_principal))\n",
    "    # Ordenar la lista por recuento de usuarios en orden descendente y tomar las 10 primeras entradas\n",
    "    resultado = sorted(fechas_usuarios_principales, key=lambda x: usuarios_por_fecha[x[0]][x[1]], reverse=True)[:10]\n",
    "    return resultado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Los top 10 emojis más usados con su respectivo conteo.\n",
    "- Enfoque: Tiempo de ejecucion optimizado\n",
    "- Descripción: Este código define una función llamada q2_time que procesa un archivo. Primero, verifica si existe un archivo Parquet con el mismo nombre que el archivo JSON de entrada. Si existe, lo lee; de lo contrario, lee el archivo JSON, extrae los emojis de los contenidos de los tweets, cuenta la frecuencia de cada emoji y devuelve una lista de los 10 emojis más comunes junto con sus recuentos. El resultado final es una lista de tuplas que contienen el emoji y su frecuencia de aparición en los tweets del archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Crea el nombre del archivo Parquet reemplazando la extensión .json por .parquet\n",
    "    archivo_parquet = file_path.replace('.json', '.parquet')\n",
    "    \n",
    "    # Verifica si existe el archivo Parquet\n",
    "    if os.path.exists(archivo_parquet):\n",
    "        # Lee el archivo Parquet si existe\n",
    "        df = pd.read_parquet(archivo_parquet)\n",
    "    else:\n",
    "        # Lee el archivo JSON si el Parquet no existe y lo guarda como Parquet\n",
    "        df = pd.read_json(file_path, lines=True)\n",
    "        df.to_parquet(archivo_parquet, index=False)\n",
    "    \n",
    "    # Inicializa una lista vacía para almacenar todos los emojis en el contenido de los tweets\n",
    "    todos_emojis = []\n",
    "    \n",
    "    # Itera a través de la columna 'content' del DataFrame\n",
    "    for contenido in df['content']:\n",
    "        # Extrae los emojis de cada contenido y los agrega a la lista de todos los emojis\n",
    "        emojis_en_contenido = [entrada['emoji'] for entrada in emoji.emoji_list(contenido)]\n",
    "        todos_emojis.extend(emojis_en_contenido)\n",
    "    \n",
    "    # Cuenta la frecuencia de cada emoji\n",
    "    conteo_emojis = Counter(todos_emojis)\n",
    "    \n",
    "    # Encuentra los 10 emojis más comunes\n",
    "    top_emojis = conteo_emojis.most_common(10)\n",
    "    \n",
    "    return top_emojis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Enfoque: Memoria en uso optimizada\n",
    " - Descripción: Este código define una función llamada q2_memory que procesa un archivo de texto. Lee cada línea del archivo, interpreta cada línea como un objeto JSON que representa un tweet y cuenta la frecuencia de cada emoji en el contenido de los tweets. Luego, devuelve una lista de los 10 emojis más comunes junto con sus recuentos. El resultado final es una lista de tuplas que contienen el emoji y su frecuencia de aparición en los tweets del archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Inicializa un contador para contar emojis\n",
    "    conteo_emojis = Counter()\n",
    "    \n",
    "    # Abre el archivo en modo lectura\n",
    "    with open(file_path, 'r') as archivo:\n",
    "        # Itera sobre cada línea en el archivo\n",
    "        for linea in archivo:\n",
    "            # Carga cada línea como un objeto JSON\n",
    "            tweet = json.loads(linea)\n",
    "            \n",
    "            # Obtiene el contenido del tweet o una cadena vacía si no hay contenido\n",
    "            contenido = tweet.get('content', '')\n",
    "            \n",
    "            # Extrae los emojis del contenido del tweet y actualiza el contador de emojis\n",
    "            emojis_en_contenido = [entrada['emoji'] for entrada in emoji.emoji_list(contenido)]\n",
    "            conteo_emojis.update(emojis_en_contenido)\n",
    "    \n",
    "    # Encuentra los 10 emojis más comunes\n",
    "    top_emojis = conteo_emojis.most_common(10)\n",
    "    \n",
    "    return top_emojis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. El top 10 histórico de usuarios (username) más influyentes en función del conteo de las menciones (@) que registra cada uno de ellos. \n",
    "- Enfoque: Tiempo de ejecucion optimizado\n",
    "- Descripción: Este código define una función llamada q3_time que procesa un archivo. Primero, verifica si existe un archivo Parquet con el mismo nombre que el archivo JSON de entrada. Si existe, lo lee; de lo contrario, lee el archivo JSON, encuentra menciones en el contenido de los tweets, cuenta la frecuencia de cada mención y devuelve una lista de las 10 menciones más frecuentes junto con sus recuentos. El resultado final es una lista de tuplas que contienen el nombre de usuario mencionado y la frecuencia de aparición en los tweets del archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Crea el nombre del archivo Parquet reemplazando la extensión .json por .parquet\n",
    "    archivo_parquet = file_path.replace('.json', '.parquet')\n",
    "    \n",
    "    # Verifica si existe el archivo Parquet\n",
    "    if os.path.exists(archivo_parquet):\n",
    "        # Lee el archivo Parquet si existe\n",
    "        df = pd.read_parquet(archivo_parquet)\n",
    "    else:\n",
    "        # Lee el archivo JSON si el Parquet no existe y lo guarda como Parquet\n",
    "        df = pd.read_json(file_path, lines=True)\n",
    "        df.to_parquet(archivo_parquet, index=False)\n",
    "    \n",
    "    # Encuentra menciones en el contenido de los tweets y crea una nueva columna 'mentions'\n",
    "    df['menciones'] = df['content'].str.findall(r'@(\\w+)')\n",
    "    \n",
    "    # Divide las filas del DataFrame por cada mención, creando múltiples filas duplicadas\n",
    "    df = df.explode('menciones', ignore_index=True)\n",
    "    \n",
    "    # Calcula la frecuencia de menciones y crea un nuevo DataFrame con columnas 'nombre_de_usuario' y 'conteo'\n",
    "    conteo_menciones = df['menciones'].value_counts().reset_index()\n",
    "    conteo_menciones.columns = ['nombre_de_usuario', 'conteo']\n",
    "    \n",
    "    # Obtiene las 10 menciones más frecuentes como un iterable de tuplas sin índice\n",
    "    top_menciones = conteo_menciones.head(10).itertuples(index=False, name=None)\n",
    "    \n",
    "    # Convierte el iterable en una lista de tuplas\n",
    "    return list(top_menciones)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Enfoque: Memoria en uso optimizada\n",
    " - Descripción: Este código define una función llamada q3_memory que procesa un archivo de texto. Lee cada línea del archivo, intenta cargarla como un objeto JSON (capturando excepciones JSONDecodeError para líneas inválidas), busca menciones en el contenido de los tweets y cuenta la frecuencia de cada mención. Luego, devuelve una lista de las 10 menciones más frecuentes junto con sus recuentos. El resultado final es una lista de tuplas que contienen el nombre de usuario mencionado y la frecuencia de aparición en los tweets del archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Inicializa un contador para contar menciones\n",
    "    conteo_menciones = Counter()\n",
    "    \n",
    "    # Abre el archivo en modo lectura\n",
    "    with open(file_path, 'r') as archivo:\n",
    "        # Itera sobre cada línea en el archivo\n",
    "        for linea in archivo:\n",
    "            try:\n",
    "                # Intenta cargar cada línea como un objeto JSON (puede haber excepciones JSONDecodeError)\n",
    "                tweet = json.loads(linea)\n",
    "                \n",
    "                # Obtiene el contenido del tweet o una cadena vacía si no hay contenido\n",
    "                contenido = tweet.get('content', '')\n",
    "                \n",
    "                # Encuentra menciones en el contenido del tweet\n",
    "                menciones = re.findall(r'@(\\w+)', contenido)\n",
    "                \n",
    "                # Actualiza el contador de menciones con las menciones encontradas\n",
    "                conteo_menciones.update(menciones)\n",
    "            except json.JSONDecodeError:\n",
    "                # Captura excepciones JSONDecodeError en caso de líneas inválidas\n",
    "                pass\n",
    "    \n",
    "    # Obtiene las 10 menciones más frecuentes\n",
    "    top_menciones = conteo_menciones.most_common(10)\n",
    "    \n",
    "    return top_menciones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
